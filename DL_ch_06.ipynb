{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_ch.06.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4Km69ube49FUhM5G/RZrB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimkim1029/Esther-Hahyeon-Kim/blob/main/DL_ch_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUU4xmLLlFaM",
        "outputId": "7bbdaff1-25dc-4391-e1d1-9599441015e7"
      },
      "source": [
        "# 2개의 층을 연결합니다 - 다층 신경망 \n",
        "# 선형회귀, 로지스틱 회귀, 단일층 신경망, 경사하강법, 과대적합, 과소적합 \n",
        "\n",
        "# 06-1. 신경망 알고리즘을 벡터화하여 한 번에 전체 샘플을 사용합니다. \n",
        "\n",
        "# 벡터화된 연산은 알고리즘의 성능을 올립니다. - 넘파이, 머신러닝, 딥러닝 패키지 \n",
        "# 행렬 연산을 빠르게 수행 - 벡터화된 연산 - 알고리즘 성능 향상 \n",
        "\n",
        "\n",
        "# 배치 경사 하강법으로 성능을 올립니다. \n",
        "# 경사하강법 알고리즘 - 선형 회귀, 로지스틱 회귀, - 확률적 경사하강법 - 알고리즘 1번 & 1개 샘플 \n",
        "# 확률적 경사하강법 : 손실함수의 전역 최솟값을 불안정하게 찾습니다. \n",
        "\n",
        "# 벡터화된 연산과 행렬 연산을 알아봅니다. \n",
        "# 점 곱을 알아봅니다. \n",
        "# 점 곱을 행렬 곱셈으로 표현합니다. \n",
        "# 전체 샘플에 대한 가중치 곱의 합을 행렬 곱셈으로 구합니다. \n",
        "\n",
        "# SingleLayer 클래스에 배치 경사 하강법 적용하기 \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "x = cancer.data\n",
        "y = cancer.target\n",
        "x_train_all, x_test, y_train_all, y_test = train_test_split(x,y,stratify=y, test_size=0.2, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)\n",
        "\n",
        "print(x_train.shape, x_val.shape)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(364, 30) (91, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAaN3yKupAkQ"
      },
      "source": [
        "# 그레이디언트 계산 이해하기 \n",
        "# 그레이디언트 = 오차와 입력 데이터의 곱 \n",
        "# forpass(), backprop() 메서드에 배치 경사 하강법 적용하기 \n",
        "\n",
        "def forpass(self, x):\n",
        "  z = np.dot(x, self.w) + self.b\n",
        "  return z \n",
        "\n",
        "def backprop(self, x, err):\n",
        "  m = len(x)\n",
        "  w_grad = np.dot(x.T, err) / m\n",
        "  b_grad = np.sum(err) / m\n",
        "  return w_grad, b_grad "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOzvIW4dqRaF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}